{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "282a1f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.38.0)\n",
      "Collecting selenium-wire\n",
      "  Downloading selenium_wire-5.1.0-py3-none-any.whl.metadata (49 kB)\n",
      "Collecting selenium-stealth\n",
      "  Downloading selenium_stealth-1.0.6-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: undetected-chromedriver in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.5.5)\n",
      "Requirement already satisfied: webdriver-manager in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from selenium) (2025.10.5)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from selenium) (1.9.0)\n",
      "Collecting blinker>=1.4 (from selenium-wire)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting brotli>=1.0.9 (from selenium-wire)\n",
      "  Downloading Brotli-1.1.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (5.5 kB)\n",
      "Collecting kaitaistruct>=0.7 (from selenium-wire)\n",
      "  Downloading kaitaistruct-0.11-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyasn1>=0.3.1 (from selenium-wire)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pyOpenSSL>=22.0.0 (from selenium-wire)\n",
      "  Downloading pyopenssl-25.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: pyparsing>=2.4.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from selenium-wire) (3.2.3)\n",
      "Requirement already satisfied: pysocks>=1.7.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from selenium-wire) (1.7.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from selenium-wire) (1.2.0)\n",
      "Collecting zstandard>=0.14.1 (from selenium-wire)\n",
      "  Downloading zstandard-0.25.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting h2>=4.0 (from selenium-wire)\n",
      "  Downloading h2-4.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting hyperframe>=6.0 (from selenium-wire)\n",
      "  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from undetected-chromedriver) (2.32.5)\n",
      "Requirement already satisfied: websockets in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from undetected-chromedriver) (15.0.1)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from webdriver-manager) (1.2.1)\n",
      "Requirement already satisfied: packaging in /Users/mkbs/Library/Python/3.13/lib/python/site-packages (from webdriver-manager) (25.0)\n",
      "Collecting hpack<5,>=4.1 (from h2>=4.0->selenium-wire)\n",
      "  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting cryptography<47,>=45.0.7 (from pyOpenSSL>=22.0.0->selenium-wire)\n",
      "  Downloading cryptography-46.0.3-cp311-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (3.11)\n",
      "Requirement already satisfied: outcome in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from wsproto>=0.14->selenium-wire) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->undetected-chromedriver) (3.4.4)\n",
      "Collecting cffi>=2.0.0 (from cryptography<47,>=45.0.7->pyOpenSSL>=22.0.0->selenium-wire)\n",
      "  Downloading cffi-2.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography<47,>=45.0.7->pyOpenSSL>=22.0.0->selenium-wire)\n",
      "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Downloading selenium_wire-5.1.0-py3-none-any.whl (239 kB)\n",
      "Downloading selenium_stealth-1.0.6-py3-none-any.whl (32 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading Brotli-1.1.0-cp313-cp313-macosx_10_13_universal2.whl (815 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.7/815.7 kB\u001b[0m \u001b[31m433.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h2-4.3.0-py3-none-any.whl (61 kB)\n",
      "Downloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Downloading kaitaistruct-0.11-py2.py3-none-any.whl (11 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyopenssl-25.3.0-py3-none-any.whl (57 kB)\n",
      "Downloading zstandard-0.25.0-cp313-cp313-macosx_11_0_arm64.whl (640 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m640.4/640.4 kB\u001b[0m \u001b[31m177.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-46.0.3-cp311-abi3-macosx_10_9_universal2.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m154.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading cffi-2.0.0-cp313-cp313-macosx_11_0_arm64.whl (181 kB)\n",
      "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Installing collected packages: brotli, zstandard, pycparser, pyasn1, kaitaistruct, hyperframe, hpack, blinker, h2, cffi, cryptography, pyOpenSSL, selenium-wire, selenium-stealth\n",
      "Successfully installed blinker-1.9.0 brotli-1.1.0 cffi-2.0.0 cryptography-46.0.3 h2-4.3.0 hpack-4.1.0 hyperframe-6.1.0 kaitaistruct-0.11 pyOpenSSL-25.3.0 pyasn1-0.6.1 pycparser-2.23 selenium-stealth-1.0.6 selenium-wire-5.1.0 zstandard-0.25.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium selenium-wire selenium-stealth undetected-chromedriver webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b56b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraper is running with a stealth configuration!\n",
      "Using IP from proxy and User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from selenium_stealth import stealth\n",
    "from selenium import webdriver\n",
    "# --- 1. User-Agent Setup ---\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\",\n",
    "]\n",
    "random_user_agent = random.choice(USER_AGENTS)\n",
    "\n",
    "# --- 2. Proxy Setup ---\n",
    "# Note: Replace USERNAME and PASSWORD with your actual credentials\n",
    "#proxy_options = {\n",
    "#    'proxy': {\n",
    "#        'no_proxy': 'localhost,127.0.0.1' # Bypasses proxy for local traffic\n",
    "#    }\n",
    "#}\n",
    "\n",
    "# --- 3. Chrome Options Setup ---\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(f\"--user-agent={random_user_agent}\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "# chrome_options.add_argument(\"--headless\") # Commented out to see the browser\n",
    "chrome_options.add_argument(\"--lang=en-US,en;q=0.9\")\n",
    "\n",
    "# Basic anti-detection options\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "# Load only DOM content\n",
    "#chrome_options.page_load_strategy = 'eager' \n",
    "\n",
    "# Initialize driver with selenium-wire\n",
    "driver = webdriver.Chrome(\n",
    "    options=chrome_options,\n",
    "    #seleniumwire_options=proxy_options\n",
    ")\n",
    "\n",
    "# --- 4. Stealth Setup ---\n",
    "stealth(driver,\n",
    "        languages=[\"en-US\", \"en\"],\n",
    "        vendor=\"Google Inc.\",\n",
    "        platform=\"Win32\",\n",
    "        webgl_vendor=\"Intel Inc.\",\n",
    "        renderer=\"Intel Iris OpenGL Engine\",\n",
    "        fix_hairline=True,\n",
    "        )\n",
    "\n",
    "# --- 5. Run the Scraper ---\n",
    "print(\"Scraper is running with a stealth configuration!\")\n",
    "print(f\"Using IP from proxy and User-Agent: {random_user_agent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b10df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "import json\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdef7c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Refactored Store Scraping Functions with Try-Catch ####\n",
    "\n",
    "def get_filtered_products(products_details, word_to_search):\n",
    "    \"\"\"\n",
    "    Filter products for relevance based on search term\n",
    "    Returns: list of relevant products\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    search_term_lower = word_to_search.lower()\n",
    "    \n",
    "    for p in products_details:\n",
    "        title_lower = p[\"name\"].lower()\n",
    "        if search_term_lower in title_lower:\n",
    "            filtered.append(p)\n",
    "            \n",
    "    return filtered\n",
    "\n",
    "\n",
    "def scrape_al_fateh(driver, word_to_search, wait_time=10):\n",
    "    \"\"\"\n",
    "    Scrape Al-Fatah store for products\n",
    "    Returns: list of products with store name, or empty list on error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        store_name = \"Al-Fateh\"\n",
    "        AL_FATEH_GROCERY_URL = f\"https://alfatah.pk/search?q={word_to_search}\"\n",
    "        \n",
    "        driver.get(AL_FATEH_GROCERY_URL)\n",
    "        wait = WebDriverWait(driver, wait_time)\n",
    "        \n",
    "        product_cards = wait.until(EC.presence_of_all_elements_located(\n",
    "            (By.CSS_SELECTOR, \".col-6.col-sm-4.col-md-3.col-lg-2\")\n",
    "        ))\n",
    "        \n",
    "        products_details = []\n",
    "        for product in product_cards:\n",
    "            try:\n",
    "                a_element = product.find_element(By.CSS_SELECTOR, \"a[class='product-title-ellipsis']\")\n",
    "                product_link = a_element.get_attribute(\"href\")\n",
    "                product_name = a_element.text\n",
    "                product_price= product.find_element(By.CLASS_NAME, \"product-price\").text\n",
    "                products_details.append({\n",
    "                    \"store\": store_name,\n",
    "                    \"name\": product_name,\n",
    "                    \"link\": product_link,\n",
    "                    \"price\": product_price\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"[{store_name}] Error extracting product: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Filter for relevance\n",
    "        filtered_products = get_filtered_products(products_details, word_to_search)\n",
    "        \n",
    "        print(f\"[{store_name}] Found {len(filtered_products)} relevant products\")\n",
    "        return filtered_products\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[Al-Fateh] Error during scraping: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def scrape_metro(driver, word_to_search, wait_time=10):\n",
    "    \"\"\"\n",
    "    Scrape Metro store for products with name and price\n",
    "    Returns: list of products with store name, or empty list on error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        store_name = \"Metro\"\n",
    "        METRO_GROCERY_URL = f\"https://www.metro-online.pk/search/{word_to_search}?searchText={word_to_search}\"\n",
    "        \n",
    "        driver.get(METRO_GROCERY_URL)\n",
    "        wait = WebDriverWait(driver, wait_time)\n",
    "        \n",
    "        # Search for product\n",
    "        #input_box = wait.until(EC.presence_of_element_located(\n",
    "         #   (By.CLASS_NAME, \"newNavbar_nav_search__LBtcn\")\n",
    "        #))\n",
    "        #input_box.clear()\n",
    "       # input_box.send_keys(word_to_search)\n",
    "        #input_box.send_keys(Keys.RETURN)\n",
    "        \n",
    "        # Get product cards\n",
    "        product_cards = wait.until(EC.presence_of_all_elements_located(\n",
    "            (By.CLASS_NAME, \"CategoryGrid_product_card__FUMXW\")\n",
    "        ))\n",
    "\n",
    "        products_details = []\n",
    "\n",
    "        for product_card in product_cards:\n",
    "            try:\n",
    "                product_link=product_card.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                name = product_card.find_element(By.CLASS_NAME, \"CategoryGrid_product_name__3nYsN\").text\n",
    "                price = product_card.find_element(By.CLASS_NAME, \"CategoryGrid_product_price__Svf8T\").text\n",
    "                products_details.append({\n",
    "                        \"store\": store_name,\n",
    "                        \"name\": name,\n",
    "                        \"link\": product_link,\n",
    "                        \"price\": price\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"[{store_name}] Error extracting product details: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        # Filter for relevance\n",
    "        filtered_products = get_filtered_products(products_details, word_to_search)\n",
    "\n",
    "        print(f\"[{store_name}] Found {len(filtered_products)} products\")\n",
    "        return filtered_products\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Metro] Error during scraping: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def scrape_jalalsons(driver, word_to_search, wait_time=10):\n",
    "    \"\"\"\n",
    "    Scrape Jalal Sons store for products with name and price\n",
    "    Returns: list of products with store name, or empty list on error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        store_name = \"Jalal Sons\"\n",
    "        JALALSONS_GROCERY_URL = f\"https://jalalsons.com.pk/shop?query={word_to_search}\"\n",
    "        \n",
    "        driver.get(JALALSONS_GROCERY_URL)\n",
    "        wait = WebDriverWait(driver, wait_time)\n",
    "        \n",
    "        # Close banner if present\n",
    "        try:\n",
    "            banner_close_button = wait.until(EC.element_to_be_clickable(\n",
    "                (By.CSS_SELECTOR, \".cursor-pointer.ms-auto\")\n",
    "            ))\n",
    "            banner_close_button.click()\n",
    "        except TimeoutException:\n",
    "            print(f\"[{store_name}] No banner appeared\")\n",
    "        \n",
    "        # Select location from dropdown\n",
    "        try:\n",
    "            from selenium.webdriver.support.ui import Select\n",
    "            location_dropdown = wait.until(EC.presence_of_element_located(\n",
    "                (By.ID, \"selectDeliveryBranch\")\n",
    "            ))\n",
    "            select_object = Select(location_dropdown)\n",
    "            all_options = select_object.options\n",
    "            \n",
    "            enabled_options = [\n",
    "                opt for opt in all_options\n",
    "                if opt.is_enabled() and opt.get_attribute('value') != \"\"\n",
    "            ]\n",
    "            \n",
    "            if enabled_options:\n",
    "                random_option = random.choice(enabled_options)\n",
    "                select_object.select_by_visible_text(random_option.text)\n",
    "                \n",
    "                try:\n",
    "                    submit_button = driver.find_element(By.CLASS_NAME, \"current_loc_pop_btn\")\n",
    "                    submit_button.click()\n",
    "                except Exception as e:\n",
    "                    print(f\"[{store_name}] No button to confirm location selection: {str(e)}\")\n",
    "        except:\n",
    "            print(f\"[{store_name}] No location box appeared\")\n",
    "        \n",
    "        # Get products\n",
    "        product_cards = wait.until(EC.presence_of_all_elements_located(\n",
    "            (By.CLASS_NAME, \"single_product_theme\")\n",
    "        ))\n",
    "\n",
    "        products_details = []\n",
    "\n",
    "        for product_card in product_cards:\n",
    "            try:\n",
    "                product_link = product_card.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                name = product_card.find_element(By.CLASS_NAME, \"product_name_theme\").text\n",
    "                \n",
    "                currency = product_card.find_element(By.CLASS_NAME, \"item-currency\").text\n",
    "                value = product_card.find_element(By.CLASS_NAME, \"price-value\").text\n",
    "                price = f\"{currency} {value.strip()}\"\n",
    "                \n",
    "                products_details.append({\n",
    "                    \"store\": store_name,\n",
    "                    \"name\": name,\n",
    "                    \"link\": product_link,\n",
    "                    \"price\": price\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"[{store_name}] Error extracting product details: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        filtered_products = get_filtered_products(products_details, word_to_search)\n",
    "        print(f\"[{store_name}] Found {len(filtered_products)} products\")\n",
    "        return filtered_products\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Jalal Sons] Error during scraping: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def scrape_carrefour(driver, word_to_search, wait_time=10):\n",
    "    \"\"\"\n",
    "    Scrape Carrefour store for products with name and price\n",
    "    Returns: list of products with store name, or empty list on error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        store_name = \"Carrefour\"\n",
    "        CAREFOUR_GROCERY_URL = f\"https://www.carrefour.pk/mafpak/en/search?keyword={word_to_search}\"\n",
    "        \n",
    "        driver.get(CAREFOUR_GROCERY_URL)\n",
    "        wait = WebDriverWait(driver, wait_time)\n",
    "        \n",
    "        product = wait.until(EC.presence_of_element_located(\n",
    "            (By.XPATH, \"/html/body/div[1]/main/div/div[2]/div[2]/div/div[2]/div/div\")\n",
    "        ))\n",
    "        \n",
    "        \n",
    "        \n",
    "        product_links = set()\n",
    "        for link in product.find_elements(By.TAG_NAME, \"a\"):\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href:\n",
    "                product_links.add(href)\n",
    "\n",
    "        products_details = []\n",
    "        for link in product_links:\n",
    "            try:\n",
    "                driver.get(link)\n",
    "                name = driver.find_element(\n",
    "                    By.XPATH, \"/html/body/div[1]/main/div/div[3]/div/div[2]/h1\"\n",
    "                ).text\n",
    "                price = driver.find_element(\n",
    "                    By.XPATH, \"/html/body/div[1]/main/div/div[3]/div/div[3]/div[1]/div[1]\"\n",
    "                ).text\n",
    "                \n",
    "                products_details.append({\n",
    "                    \"store\": store_name,\n",
    "                    \"name\": name,\n",
    "                    \"link\": link,\n",
    "                    \"price\": price\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"[{store_name}] Error extracting product details: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        filtered_products = get_filtered_products(products_details, word_to_search)\n",
    "        print(f\"[{store_name}] Found {len(filtered_products)} products\")\n",
    "        return filtered_products\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[Carrefour] Error during scraping: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def scrape_imtiaz(driver, word_to_search, wait_time=5):\n",
    "    store_name = \"Imtiaz\"\n",
    "    IMTIAZ_GROCERY_URL = f\"https://shop.imtiaz.com.pk/search?q={word_to_search}\"\n",
    "    driver.get(IMTIAZ_GROCERY_URL)\n",
    "    wait = WebDriverWait(driver, wait_time)\n",
    "\n",
    "    products_details = []\n",
    "    # Select location\n",
    "    try:\n",
    "            area = wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, \"/html/body/div[2]/div[3]/div/div/div/div/div[3]/div[3]/div/div/input\")\n",
    "            ))\n",
    "            area.send_keys(Keys.ENTER)\n",
    "            area.send_keys(Keys.DOWN)\n",
    "            area.send_keys(Keys.DOWN)\n",
    "            area.send_keys(Keys.ENTER)\n",
    "            \n",
    "            submit_button = wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, \"/html/body/div[2]/div[3]/div/div/div/div/div[3]/button\")\n",
    "            ))\n",
    "            submit_button.click()\n",
    "    except TimeoutException:\n",
    "            print(f\"[{store_name}] No location box appeared\")    \n",
    "    try:\n",
    "        # Wait for product containers\n",
    "        products = wait.until(EC.presence_of_all_elements_located(\n",
    "            (By.CSS_SELECTOR, \".hazle-product-item_product_item__FSm1N\")\n",
    "        ))\n",
    "\n",
    "        for product in products:\n",
    "            try:\n",
    "                name = product.find_element(By.CLASS_NAME,\n",
    "                    \"hazle-product-item_product_item_title__wK9IT\").text\n",
    "                price = product.find_element(By.CLASS_NAME,\n",
    "                    \"hazle-product-item_product_item_price_label__ET_we\").text\n",
    "                try:\n",
    "                    link = product.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                except:\n",
    "                    link = \"\"  # if no link found\n",
    "                products_details.append({\n",
    "                    \"store\": store_name,\n",
    "                    \"name\": name,\n",
    "                    \"price\": price,\n",
    "                    \"link\": link\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"[{store_name}] Error extracting product: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"[{store_name}] Found {len(products_details)} products\")\n",
    "        return products_details\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f\"[{store_name}] No products found\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ===== MAIN SCRAPING FUNCTION =====\n",
    "def scrape_all_stores(driver, word_to_search):\n",
    "    \"\"\"\n",
    "    Scrape all 5 stores and combine results into a single list\n",
    "    Returns: list of all products from all stores with store names\n",
    "    \"\"\"\n",
    "    all_products = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting scraping for: '{word_to_search}'\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Scrape each store\n",
    "    stores_scrapers = [\n",
    "        (\"Al-Fateh\", scrape_al_fateh),\n",
    "        (\"Metro\", scrape_metro),\n",
    "        (\"Jalal Sons\", scrape_jalalsons),\n",
    "        (\"Carrefour\", scrape_carrefour),\n",
    "        (\"Imtiaz\", scrape_imtiaz),\n",
    "    ]\n",
    "    \n",
    "    for store_label, scraper_func in stores_scrapers:\n",
    "        print(f\"\\n[SCRAPING {store_label.upper()}]\")\n",
    "        try:\n",
    "            products = scraper_func(driver, word_to_search)\n",
    "            all_products.extend(products)\n",
    "        except Exception as e:\n",
    "            print(f\"FATAL ERROR for {store_label}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping Complete!\")\n",
    "    print(f\"Total products collected: {len(all_products)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return all_products\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d815d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_products_to_csv(products_list, filename=\"all_products.csv\"):\n",
    "    \"\"\"\n",
    "    Save scraped products to a CSV file.\n",
    "    Each product dictionary should have: 'store', 'name', 'price', optional 'link'.\n",
    "    \"\"\"\n",
    "    if not products_list:\n",
    "        print(\"No products to save!\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(products_list)\n",
    "\n",
    "    # Keep only required columns and rename\n",
    "    df = df[[\"store\", \"name\", \"price\"]]\n",
    "    df.columns = [\"Store\", \"Product Name\", \"Price\"]\n",
    "\n",
    "    df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ Saved {len(df)} products to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1da1760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting scraping for: 'pepsi'\n",
      "============================================================\n",
      "\n",
      "\n",
      "[SCRAPING AL-FATEH]\n",
      "[Al-Fateh] Found 13 relevant products\n",
      "\n",
      "[SCRAPING METRO]\n",
      "[Metro] Found 3 products\n",
      "\n",
      "[SCRAPING JALAL SONS]\n",
      "[Jalal Sons] No banner appeared\n",
      "[Jalal Sons] No location box appeared\n",
      "[Jalal Sons] Found 6 products\n",
      "\n",
      "[SCRAPING CARREFOUR]\n",
      "[Carrefour] Found 15 products\n",
      "\n",
      "[SCRAPING IMTIAZ]\n",
      "[Imtiaz] No location box appeared\n",
      "[Imtiaz] Found 6 products\n",
      "\n",
      "============================================================\n",
      "Scraping Complete!\n",
      "Total products collected: 43\n",
      "============================================================\n",
      "\n",
      "✅ Saved 43 products to 'products_combined.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scrape all stores for a product,\n",
    "all_products = scrape_all_stores(driver, \"pepsi\")\n",
    "\n",
    "# Save to CSV\n",
    "save_products_to_csv(all_products, \"products_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db04c199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mkbs/Library/Containers/net.whatsapp.WhatsApp/Data/tmp/documents/A750022F-709E-4DA9-93F0-520BABAD89D3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
