{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "282a1f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install selenium selenium-wire selenium-stealth undetected-chromedriver webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b56b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraper is running with a stealth configuration!\n",
      "Using IP from proxy and User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from selenium_stealth import stealth\n",
    "from selenium import webdriver\n",
    "# --- 1. User-Agent Setup ---\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\",\n",
    "]\n",
    "random_user_agent = random.choice(USER_AGENTS)\n",
    "\n",
    "# --- 2. Proxy Setup ---\n",
    "# Note: Replace USERNAME and PASSWORD with your actual credentials\n",
    "#proxy_options = {\n",
    "#    'proxy': {\n",
    "#        'no_proxy': 'localhost,127.0.0.1' # Bypasses proxy for local traffic\n",
    "#    }\n",
    "#}\n",
    "\n",
    "# --- 3. Chrome Options Setup ---\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(f\"--user-agent={random_user_agent}\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "# chrome_options.add_argument(\"--headless\") # Commented out to see the browser\n",
    "chrome_options.add_argument(\"--lang=en-US,en;q=0.9\")\n",
    "\n",
    "# Basic anti-detection options\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "# Load only DOM content\n",
    "#chrome_options.page_load_strategy = 'eager' \n",
    "\n",
    "# Initialize driver with selenium-wire\n",
    "driver = webdriver.Chrome(\n",
    "    options=chrome_options,\n",
    "    #seleniumwire_options=proxy_options\n",
    ")\n",
    "\n",
    "# --- 4. Stealth Setup ---\n",
    "stealth(driver,\n",
    "        languages=[\"en-US\", \"en\"],\n",
    "        vendor=\"Google Inc.\",\n",
    "        platform=\"Win32\",\n",
    "        webgl_vendor=\"Intel Inc.\",\n",
    "        renderer=\"Intel Iris OpenGL Engine\",\n",
    "        fix_hairline=True,\n",
    "        )\n",
    "\n",
    "# --- 5. Run the Scraper ---\n",
    "print(\"Scraper is running with a stealth configuration!\")\n",
    "print(f\"Using IP from proxy and User-Agent: {random_user_agent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b10df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c6512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_filtered_products(products_details, word_to_search):\n",
    "    \"\"\"\n",
    "    Filter products for relevance based on search term\n",
    "    Returns: list of relevant products\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    search_term_lower = word_to_search.lower()\n",
    "    \n",
    "    for p in products_details:\n",
    "        title_lower = p[\"name\"].lower()\n",
    "        if search_term_lower in title_lower:\n",
    "            filtered.append(p)\n",
    "            \n",
    "    return filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdef7c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Refactored Store Scraping Functions with Try-Catch ####\n",
    "\n",
    "\n",
    "\n",
    "def scrape_al_fateh(driver, word_to_search, wait_time=10):\n",
    "    \"\"\"\n",
    "    Scrape Al-Fatah store for products\n",
    "    Returns: list of products with store name, or empty list on error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        store_name = \"Al-Fateh\"\n",
    "        AL_FATEH_GROCERY_URL = f\"https://alfatah.pk/search?q={word_to_search}\"\n",
    "        \n",
    "        driver.get(AL_FATEH_GROCERY_URL)\n",
    "        wait = WebDriverWait(driver, wait_time)\n",
    "        \n",
    "        product_cards = wait.until(EC.presence_of_all_elements_located(\n",
    "            (By.CSS_SELECTOR, \".col-6.col-sm-4.col-md-3.col-lg-2\")\n",
    "        ))\n",
    "        \n",
    "        products_details = []\n",
    "        for product in product_cards:\n",
    "            try:\n",
    "                a_element = product.find_element(By.CSS_SELECTOR, \"a[class='product-title-ellipsis']\")\n",
    "                product_link = a_element.get_attribute(\"href\")\n",
    "                product_name = a_element.text\n",
    "                product_price= product.find_element(By.CLASS_NAME, \"product-price\").text\n",
    "                image_container=product.find_element(By.CLASS_NAME, \"image\")\n",
    "                image_url=image_container.find_element(By.TAG_NAME, \"img\").get_attribute(\"src\")\n",
    "\n",
    "                \n",
    "                products_details.append({\n",
    "                    \"store\": store_name,\n",
    "                    \"name\": product_name,\n",
    "                    \"product-link\": product_link,\n",
    "                    \"price\": product_price,\n",
    "                    \"image_url\": image_url\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"[{store_name}] Error extracting product: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Filter for relevance\n",
    "        filtered_products = get_filtered_products(products_details, word_to_search)\n",
    "        \n",
    "        print(f\"[{store_name}] Found {len(filtered_products)} relevant products\")\n",
    "        return filtered_products\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[Al-Fateh] Error during scraping: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def scrape_metro(driver, word_to_search, wait_time=10):\n",
    "    try:\n",
    "        store_name = \"Metro\"\n",
    "        METRO_GROCERY_URL = f\"https://www.metro-online.pk/search/{word_to_search}?searchText={word_to_search}\"\n",
    "        \n",
    "        driver.get(METRO_GROCERY_URL)\n",
    "        wait = WebDriverWait(driver, wait_time)\n",
    "        \n",
    "        product_cards = wait.until(EC.presence_of_all_elements_located(\n",
    "            (By.CLASS_NAME, \"CategoryGrid_product_card__FUMXW\")\n",
    "        ))\n",
    "\n",
    "        products_details = []\n",
    "\n",
    "        for product_card in product_cards:\n",
    "            try:\n",
    "                product_link=product_card.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                name = product_card.find_element(By.CLASS_NAME, \"CategoryGrid_product_name__3nYsN\").text\n",
    "                price = product_card.find_element(By.CLASS_NAME, \"CategoryGrid_product_price__Svf8T\").text\n",
    "                \n",
    "                image_container=product_card.find_element(By.CLASS_NAME, \"CategoryGrid_productImg_container__Ga1ll\")\n",
    "                image_url=image_container.find_element(By.TAG_NAME, \"img\").get_attribute(\"src\")\n",
    "                products_details.append({\n",
    "                        \"store\": store_name,\n",
    "                        \"name\": name,\n",
    "                        \"product-link\": product_link,\n",
    "                        \"price\": price,\n",
    "                        \"image_url\": image_url\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"[{store_name}] Error extracting product details: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        # Filter for relevance\n",
    "        filtered_products = get_filtered_products(products_details, word_to_search)\n",
    "\n",
    "        print(f\"[{store_name}] Found {len(filtered_products)} products\")\n",
    "        return filtered_products\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Metro] Error during scraping: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def scrape_jalalsons(driver, word_to_search, wait_time=10):\n",
    "    \"\"\"\n",
    "    Scrape Jalal Sons store for products with name and price\n",
    "    Returns: list of products with store name, or empty list on error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        store_name = \"Jalal Sons\"\n",
    "        JALALSONS_GROCERY_URL = f\"https://jalalsons.com.pk/shop?query={word_to_search}\"\n",
    "        \n",
    "        driver.get(JALALSONS_GROCERY_URL)\n",
    "        wait = WebDriverWait(driver, wait_time)\n",
    "        \n",
    "        # Close banner if present\n",
    "        try:\n",
    "            banner_close_button = wait.until(EC.element_to_be_clickable(\n",
    "                (By.CSS_SELECTOR, \".cursor-pointer.ms-auto\")\n",
    "            ))\n",
    "            banner_close_button.click()\n",
    "        except TimeoutException:\n",
    "            print(f\"[{store_name}] No banner appeared\")\n",
    "        \n",
    "        # Select location from dropdown\n",
    "        try:\n",
    "            from selenium.webdriver.support.ui import Select\n",
    "            location_dropdown = wait.until(EC.presence_of_element_located(\n",
    "                (By.ID, \"selectDeliveryBranch\")\n",
    "            ))\n",
    "            select_object = Select(location_dropdown)\n",
    "            all_options = select_object.options\n",
    "            \n",
    "            enabled_options = [\n",
    "                opt for opt in all_options\n",
    "                if opt.is_enabled() and opt.get_attribute('value') != \"\"\n",
    "            ]\n",
    "            \n",
    "            if enabled_options:\n",
    "                random_option = random.choice(enabled_options)\n",
    "                select_object.select_by_visible_text(random_option.text)\n",
    "                \n",
    "                try:\n",
    "                    submit_button = driver.find_element(By.CLASS_NAME, \"current_loc_pop_btn\")\n",
    "                    submit_button.click()\n",
    "                except Exception as e:\n",
    "                    print(f\"[{store_name}] No button to confirm location selection: {str(e)}\")\n",
    "        except:\n",
    "            print(f\"[{store_name}] No location box appeared\")\n",
    "        \n",
    "        # Get products\n",
    "        product_cards = wait.until(EC.presence_of_all_elements_located(\n",
    "            (By.CLASS_NAME, \"single_product_theme\")\n",
    "        ))\n",
    "\n",
    "        products_details = []\n",
    "\n",
    "        for product_card in product_cards:\n",
    "            try:\n",
    "                product_link = product_card.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                name = product_card.find_element(By.CLASS_NAME, \"product_name_theme\").text\n",
    "                \n",
    "                currency = product_card.find_element(By.CLASS_NAME, \"item-currency\").text\n",
    "                value = product_card.find_element(By.CLASS_NAME, \"price-value\").text\n",
    "                price = f\"{currency} {value.strip()}\"\n",
    "                image_url=product_card.find_element(By.TAG_NAME, \"img\").get_attribute(\"src\")\n",
    "                \n",
    "                products_details.append({\n",
    "                    \"store\": store_name,\n",
    "                    \"name\": name,\n",
    "                    \"product-link\": product_link,\n",
    "                    \"price\": price,\n",
    "                    \"image_url\":image_url\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"[{store_name}] Error extracting product details: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        filtered_products = get_filtered_products(products_details, word_to_search)\n",
    "        print(f\"[{store_name}] Found {len(filtered_products)} products\")\n",
    "        return filtered_products\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Jalal Sons] Error during scraping: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def scrape_carrefour(driver, word_to_search, wait_time=10):\n",
    "    \"\"\"\n",
    "    Scrape Carrefour store for products with name and price\n",
    "    Returns: list of products with store name, or empty list on error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        store_name = \"Carrefour\"\n",
    "        CAREFOUR_GROCERY_URL = f\"https://www.carrefour.pk/mafpak/en/search?keyword={word_to_search}\"\n",
    "        \n",
    "        driver.get(CAREFOUR_GROCERY_URL)\n",
    "        wait = WebDriverWait(driver, wait_time)\n",
    "        driver.refresh()\n",
    "        product = wait.until(EC.presence_of_element_located(\n",
    "            (By.XPATH, \"/html/body/div[1]/main/div/div[2]/div[2]/div/div[2]/div/div\")\n",
    "        ))\n",
    "        \n",
    "        \n",
    "        \n",
    "        product_links = set()\n",
    "        for link in product.find_elements(By.TAG_NAME, \"a\"):\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href:\n",
    "                product_links.add(href)\n",
    "\n",
    "        products_details = []\n",
    "        for link in product_links:\n",
    "            try:\n",
    "                driver.get(link)\n",
    "                name = driver.find_element(\n",
    "                    By.XPATH, \"/html/body/div[1]/main/div/div[3]/div/div[2]/h1\"\n",
    "                ).text\n",
    "                price = driver.find_element(\n",
    "                    By.XPATH, \"/html/body/div[1]/main/div/div[3]/div/div[3]/div[1]/div[1]\"\n",
    "                ).text\n",
    "                \n",
    "                products_details.append({\n",
    "                    \"store\": store_name,\n",
    "                    \"name\": name,\n",
    "                    \"link\": link,\n",
    "                    \"price\": price\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"[{store_name}] Error extracting product details: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        filtered_products = get_filtered_products(products_details, word_to_search)\n",
    "        print(f\"[{store_name}] Found {len(filtered_products)} products\")\n",
    "        return filtered_products\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[Carrefour] Error during scraping: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def scrape_imtiaz(driver, word_to_search, wait_time=5):\n",
    "    \"\"\"\n",
    "    Scrape Imtiaz store for products with pagination\n",
    "    Returns: list of products with store name, or empty list on error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        store_name = \"Imtiaz\"\n",
    "        IMTIAZ_GROCERY_URL = f\"https://shop.imtiaz.com.pk/search?q={word_to_search}\"\n",
    "        driver.get(IMTIAZ_GROCERY_URL)\n",
    "        wait = WebDriverWait(driver, wait_time)\n",
    "\n",
    "        products_details = []\n",
    "        \n",
    "        # Select location\n",
    "        try:\n",
    "            area = wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, \"/html/body/div[2]/div[3]/div/div/div/div/div[3]/div[3]/div/div/input\")\n",
    "            ))\n",
    "            area.send_keys(Keys.ENTER)\n",
    "            area.send_keys(Keys.DOWN)\n",
    "            area.send_keys(Keys.DOWN)\n",
    "            area.send_keys(Keys.ENTER)\n",
    "            \n",
    "            submit_button = wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, \"/html/body/div[2]/div[3]/div/div/div/div/div[3]/button\")\n",
    "            ))\n",
    "            submit_button.click()\n",
    "        except TimeoutException:\n",
    "            print(f\"[{store_name}] No location box appeared\")\n",
    "        \n",
    "        # Get initial products\n",
    "        try:\n",
    "            products = wait.until(EC.presence_of_all_elements_located(\n",
    "                (By.CLASS_NAME, \"hazle-product-item_product_item__FSm1N\")\n",
    "            ))\n",
    "            \n",
    "            current_url = driver.current_url\n",
    "            \n",
    "            # Extract products and handle pagination\n",
    "            while True:\n",
    "                try:\n",
    "                    # Wait for products to load\n",
    "                    products = wait.until(EC.presence_of_all_elements_located(\n",
    "                        (By.CLASS_NAME, \"hazle-product-item_product_item__FSm1N\")\n",
    "                    ))\n",
    "                    \n",
    "                    # Extract all products on current page\n",
    "                    for product in products:\n",
    "                        try:\n",
    "                            product_text_container = product.find_element(By.CLASS_NAME, \"hazle-product-item_product_item_text_container__Apuq1\")\n",
    "                            \n",
    "                            product_name = product_text_container.find_element(By.CLASS_NAME, \"hazle-product-item_product_item_description__ejRDa\").text.strip()\n",
    "                            product_price = product_text_container.find_element(By.CLASS_NAME, \"hazle-product-item_product_item_price_label__ET_we\").text.strip()\n",
    "                            \n",
    "                            product_link_id = product.get_attribute(\"id\")\n",
    "                            product_link = f\"https://shop.imtiaz.com.pk/product/{product_link_id}\"\n",
    "                            \n",
    "                            image_url = product.find_element(By.TAG_NAME, \"img\").get_attribute(\"src\")\n",
    "                            \n",
    "                            products_details.append({\n",
    "                                \"store\": store_name,\n",
    "                                \"name\": product_name,\n",
    "                                \"product-link\": product_link,\n",
    "                                \"price\": product_price,\n",
    "                                \"image_url\": image_url\n",
    "                            })\n",
    "                        except Exception as e:\n",
    "                            print(f\"[{store_name}] Error extracting product info: {str(e)}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Try to find and click Next button\n",
    "                    try:\n",
    "                        button = driver.find_element(By.XPATH, \"//button[normalize-space()='Next']\")\n",
    "                        \n",
    "                        if button.get_attribute(\"disabled\"):\n",
    "                            print(f\"[{store_name}] Reached last page\")\n",
    "                            break\n",
    "                        else:\n",
    "                            current_url = driver.current_url\n",
    "                            button.click()\n",
    "                            time.sleep(2)  # Wait for page to load\n",
    "                    except NoSuchElementException:\n",
    "                        print(f\"[{store_name}] Last page reached\")\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"[{store_name}] Error in pagination loop: {str(e)}\")\n",
    "                    break\n",
    "            \n",
    "            filtered_products = get_filtered_products(products_details, word_to_search)\n",
    "            print(f\"[{store_name}] Found {len(filtered_products)} products\")\n",
    "            return filtered_products\n",
    "            \n",
    "        except TimeoutException:\n",
    "            print(f\"[{store_name}] No products found\")\n",
    "            return []\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[{store_name}] Error during scraping: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ===== MAIN SCRAPING FUNCTION =====\n",
    "def scrape_all_stores(driver, word_to_search):\n",
    "    \"\"\"\n",
    "    Scrape all 5 stores and combine results into a single list\n",
    "    Returns: list of all products from all stores with store names\n",
    "    \"\"\"\n",
    "    all_products = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting scraping for: '{word_to_search}'\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Scrape each store\n",
    "    stores_scrapers = [\n",
    "        (\"Al-Fateh\", scrape_al_fateh),\n",
    "        (\"Metro\", scrape_metro),\n",
    "        (\"Jalal Sons\", scrape_jalalsons),\n",
    "        (\"Carrefour\", scrape_carrefour),\n",
    "        (\"Imtiaz\", scrape_imtiaz),\n",
    "    ]\n",
    "    \n",
    "    for store_label, scraper_func in stores_scrapers:\n",
    "        print(f\"\\n[SCRAPING {store_label.upper()}]\")\n",
    "        try:\n",
    "            products = scraper_func(driver, word_to_search)\n",
    "            all_products.extend(products)\n",
    "        except Exception as e:\n",
    "            print(f\"FATAL ERROR for {store_label}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping Complete!\")\n",
    "    print(f\"Total products collected: {len(all_products)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return all_products\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d815d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_products_to_csv(products_list, filename=\"all_products.csv\"):\n",
    "    \"\"\"\n",
    "    Save scraped products to a CSV file.\n",
    "    Each product dictionary should have: 'store', 'name', 'price', optional 'link'.\n",
    "    \"\"\"\n",
    "    if not products_list:\n",
    "        print(\"No products to save!\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(products_list)\n",
    "\n",
    "\n",
    "    df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ Saved {len(df)} products to '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807d991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1da1760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting scraping for: 'pepsi'\n",
      "============================================================\n",
      "\n",
      "\n",
      "[SCRAPING AL-FATEH]\n",
      "[Al-Fateh] Found 15 relevant products\n",
      "\n",
      "[SCRAPING METRO]\n",
      "[Metro] Found 8 products\n",
      "\n",
      "[SCRAPING JALAL SONS]\n",
      "[Jalal Sons] No banner appeared\n",
      "[Jalal Sons] Found 6 products\n",
      "\n",
      "[SCRAPING CARREFOUR]\n",
      "[Carrefour] Error during scraping: Message: \n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0x7ff6c92aa235\n",
      "\t0x7ff6c9002630\n",
      "\t0x7ff6c8d916dd\n",
      "\t0x7ff6c8dea27e\n",
      "\t0x7ff6c8dea58c\n",
      "\t0x7ff6c8e3ed77\n",
      "\t0x7ff6c8e3baba\n",
      "\t0x7ff6c8ddb0ed\n",
      "\t0x7ff6c8ddbf63\n",
      "\t0x7ff6c92d5d60\n",
      "\t0x7ff6c92cfe8a\n",
      "\t0x7ff6c92f1005\n",
      "\t0x7ff6c901d71e\n",
      "\t0x7ff6c9024e1f\n",
      "\t0x7ff6c900b7c4\n",
      "\t0x7ff6c900b97f\n",
      "\t0x7ff6c8ff18e8\n",
      "\t0x7ff9bd82e8d7\n",
      "\t0x7ff9bfaec53c\n",
      "\n",
      "\n",
      "[SCRAPING IMTIAZ]\n",
      "[Imtiaz] Last page reached\n",
      "[Imtiaz] Found 5 products\n",
      "\n",
      "============================================================\n",
      "Scraping Complete!\n",
      "Total products collected: 34\n",
      "============================================================\n",
      "\n",
      "✅ Saved 34 products to 'searched-products.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scrape all stores for a product,\n",
    "all_products = scrape_all_stores(driver, \"pepsi\")\n",
    "\n",
    "# Save to CSV\n",
    "save_products_to_csv(all_products, \"searched-products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db04c199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\UNIVERSITY\\Semester 7\\DS\\project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_venv)",
   "language": "python",
   "name": "my_venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
